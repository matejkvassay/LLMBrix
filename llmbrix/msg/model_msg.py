import io
from functools import cached_property

import PIL.Image
from google.genai import types

from llmbrix.msg.segment_types import SegmentTypes

MODEL_ROLE_NAME = "model"


class ModelMsg(types.Content):
    """
    LLM response message.

    Note this message internally contains multiple message segments.
    These segments can be TEXT, IMAGE, TOOL_CALL, FILE, THOUGHT or AUDIO.

    You can control what modalities are allowed in generation config.

    These parts are generated by LLM in order => it can generate [TEXT, IMAGE, TEXT]

    Use `.segments` property to get a list of segments in correct order to be rendered.
    Use `.text` to get concatenation of all text segments. Useful in cases where LLM is used just to output text.
    Use `.images` to get list of PIL images.

    Note `.text` might be empty in cases where tool calls are requested.

    For audio beware that both audio / text segments might be present - use `.segments` to render both in correct order.

    Code generation and outputs are not supported (even though present in Gemini API).

    In order to ensure small object size and easy "Pydantic" serialization all these properties are computed in
    lazy fashion and not registered as Pydantic object attributes.
    """

    def __init__(self, parts: list[types.Part]):
        """
        Args:
            parts: Part objects from Content object returned by Gemini API.
                   See from_text() constructor to initialize from string.
        """
        super().__init__(role=MODEL_ROLE_NAME, parts=parts)

    @classmethod
    def from_text(cls, text: str):
        """
        Initialize new ModelMsg from string.
        Creates Content object with 1 "text" part.
        Use when "faking" model message.

        Args:
            text: Content of the model message.

        Returns:
            ModelMsg instance.
        """
        return cls(parts=[types.Part.from_text(text=text)])

    @property
    def modalities(self) -> list["SegmentTypes"]:
        """
        List of modalities included in this output.

        Returns: List of str modalities included.
        """
        return list(self._inspected_parts["modalities"])

    @property
    def segments(self) -> list[dict]:
        """Ordered list of segments (Text, Images, Tools) for interleaved UI rendering."""
        return self._inspected_parts["visible_segments"]

    @property
    def text(self) -> str | None:
        """The final answer text from the model."""
        return "".join(self._inspected_parts[SegmentTypes.TEXT])

    @property
    def thought(self) -> str:
        """The internal reasoning process text."""
        return "".join(self._inspected_parts[SegmentTypes.THOUGHT])

    @property
    def tool_calls(self) -> list[types.FunctionCall]:
        return self._inspected_parts[SegmentTypes.TOOL_CALL]

    @property
    def images(self) -> list[PIL.Image.Image]:
        return self._inspected_parts[SegmentTypes.IMAGE]

    @property
    def audio_bytes(self) -> bytes:
        """Concatenated raw audio data from all parts."""
        return b"".join(self._inspected_parts[SegmentTypes.AUDIO])

    @property
    def has_audio(self) -> bool:
        return SegmentTypes.AUDIO in self.modalities

    @cached_property
    def _inspected_parts(self):
        res = {
            SegmentTypes.TEXT: [],
            SegmentTypes.TOOL_CALL: [],
            SegmentTypes.IMAGE: [],
            SegmentTypes.FILE: [],
            SegmentTypes.THOUGHT: [],
            SegmentTypes.AUDIO: [],
            "modalities": set(),
            "visible_segments": [],  # <--- Added for interleaved rendering
        }

        for part in self.parts:
            # 1. Capture Thoughts (Hidden from normal UI flow)
            if getattr(part, "thought", None):
                res[SegmentTypes.THOUGHT].append(part.text or "")
                res["modalities"].add(SegmentTypes.THOUGHT)

            # 2. Capture User-Visible Text
            elif part.text:
                res[SegmentTypes.TEXT].append(part.text)
                res["modalities"].add(SegmentTypes.TEXT)
                # Keep order for UI
                res["visible_segments"].append({"modality": SegmentTypes.TEXT, "content": part.text})

            # 3. Capture Tool Calls
            elif part.function_call:
                res[SegmentTypes.TOOL_CALL].append(part.function_call)
                res["modalities"].add(SegmentTypes.TOOL_CALL)
                # Keep order for UI
                res["visible_segments"].append({"modality": SegmentTypes.TOOL_CALL, "content": part.function_call})

            # 4. Capture Media
            elif part.inline_data:
                mime = part.inline_data.mime_type
                data = part.inline_data.data

                if mime.startswith("image/"):
                    img = PIL.Image.open(io.BytesIO(data))
                    res[SegmentTypes.IMAGE].append(img)
                    res["modalities"].add(SegmentTypes.IMAGE)
                    res["visible_segments"].append({"modality": SegmentTypes.IMAGE, "content": img})
                elif mime.startswith("audio/"):
                    res[SegmentTypes.AUDIO].append(data)
                    res["modalities"].add(SegmentTypes.AUDIO)
                    res["visible_segments"].append({"modality": SegmentTypes.AUDIO, "content": data, "mime_type": mime})
                else:
                    file_data = (data, mime)
                    res[SegmentTypes.FILE].append(file_data)
                    res["modalities"].add(SegmentTypes.FILE)
                    res["visible_segments"].append({"modality": SegmentTypes.FILE, "content": file_data})

        return res

    def __repr__(self):
        return f"<ModelMsg, modalities={[m.value for m in self.modalities]}>"
